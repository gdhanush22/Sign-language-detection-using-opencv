{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各パス指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
    "model_save_path = 'model/keypoint_classifier/keypoint_classifier.keras'\n",
    "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m860\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m210\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m110\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,180</span> (4.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,180\u001b[0m (4.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,180</span> (4.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,180\u001b[0m (4.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "# Callback for early stopping\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1406 - loss: 2.2931\n",
      "Epoch 1: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.1406 - loss: 2.2931 - val_accuracy: 0.0909 - val_loss: 2.3391\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.0938 - loss: 2.2743\n",
      "Epoch 2: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.0938 - loss: 2.2743 - val_accuracy: 0.0455 - val_loss: 2.3360\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1094 - loss: 2.2954\n",
      "Epoch 3: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.1094 - loss: 2.2954 - val_accuracy: 0.0455 - val_loss: 2.3339\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1719 - loss: 2.2519\n",
      "Epoch 4: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.1719 - loss: 2.2519 - val_accuracy: 0.0000e+00 - val_loss: 2.3323\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1250 - loss: 2.2512\n",
      "Epoch 5: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.1250 - loss: 2.2512 - val_accuracy: 0.0000e+00 - val_loss: 2.3309\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1406 - loss: 2.2578\n",
      "Epoch 6: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.1406 - loss: 2.2578 - val_accuracy: 0.0000e+00 - val_loss: 2.3294\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2031 - loss: 2.2388\n",
      "Epoch 7: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.2031 - loss: 2.2388 - val_accuracy: 0.0000e+00 - val_loss: 2.3280\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1406 - loss: 2.2356\n",
      "Epoch 8: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.1406 - loss: 2.2356 - val_accuracy: 0.0000e+00 - val_loss: 2.3265\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2031 - loss: 2.2285\n",
      "Epoch 9: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.2031 - loss: 2.2285 - val_accuracy: 0.0000e+00 - val_loss: 2.3251\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2031 - loss: 2.2467\n",
      "Epoch 10: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.2031 - loss: 2.2467 - val_accuracy: 0.0000e+00 - val_loss: 2.3235\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1562 - loss: 2.2069\n",
      "Epoch 11: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.1562 - loss: 2.2069 - val_accuracy: 0.0455 - val_loss: 2.3218\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2188 - loss: 2.1954\n",
      "Epoch 12: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.2188 - loss: 2.1954 - val_accuracy: 0.0455 - val_loss: 2.3192\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2344 - loss: 2.2180\n",
      "Epoch 13: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.2344 - loss: 2.2180 - val_accuracy: 0.0455 - val_loss: 2.3172\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1719 - loss: 2.2080\n",
      "Epoch 14: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.1719 - loss: 2.2080 - val_accuracy: 0.0455 - val_loss: 2.3151\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.1562 - loss: 2.1888\n",
      "Epoch 15: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 671ms/step - accuracy: 0.1562 - loss: 2.1888 - val_accuracy: 0.0455 - val_loss: 2.3130\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.1719 - loss: 2.1756\n",
      "Epoch 16: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.1719 - loss: 2.1756 - val_accuracy: 0.0909 - val_loss: 2.3108\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2031 - loss: 2.1624\n",
      "Epoch 17: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.2031 - loss: 2.1624 - val_accuracy: 0.0909 - val_loss: 2.3084\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1562 - loss: 2.1481\n",
      "Epoch 18: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.1562 - loss: 2.1481 - val_accuracy: 0.0909 - val_loss: 2.3061\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2656 - loss: 2.1748\n",
      "Epoch 19: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.2656 - loss: 2.1748 - val_accuracy: 0.0909 - val_loss: 2.3040\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2500 - loss: 2.1614\n",
      "Epoch 20: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.2500 - loss: 2.1614 - val_accuracy: 0.0909 - val_loss: 2.3019\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2188 - loss: 2.1515\n",
      "Epoch 21: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.2188 - loss: 2.1515 - val_accuracy: 0.0909 - val_loss: 2.2995\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1562 - loss: 2.1590\n",
      "Epoch 22: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.1562 - loss: 2.1590 - val_accuracy: 0.0909 - val_loss: 2.2968\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2500 - loss: 2.0938\n",
      "Epoch 23: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.2500 - loss: 2.0938 - val_accuracy: 0.0909 - val_loss: 2.2943\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2500 - loss: 2.1312\n",
      "Epoch 24: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.2500 - loss: 2.1312 - val_accuracy: 0.2273 - val_loss: 2.2916\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2656 - loss: 2.1450\n",
      "Epoch 25: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.2656 - loss: 2.1450 - val_accuracy: 0.2727 - val_loss: 2.2887\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2656 - loss: 2.1788\n",
      "Epoch 26: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.2656 - loss: 2.1788 - val_accuracy: 0.2727 - val_loss: 2.2858\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2656 - loss: 2.0666\n",
      "Epoch 27: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.2656 - loss: 2.0666 - val_accuracy: 0.2727 - val_loss: 2.2835\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2500 - loss: 2.0530\n",
      "Epoch 28: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.2500 - loss: 2.0530 - val_accuracy: 0.2727 - val_loss: 2.2808\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2812 - loss: 2.0361\n",
      "Epoch 29: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.2812 - loss: 2.0361 - val_accuracy: 0.2727 - val_loss: 2.2783\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2344 - loss: 2.0805\n",
      "Epoch 30: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.2344 - loss: 2.0805 - val_accuracy: 0.2727 - val_loss: 2.2759\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2656 - loss: 2.0322\n",
      "Epoch 31: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.2656 - loss: 2.0322 - val_accuracy: 0.2727 - val_loss: 2.2734\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2500 - loss: 2.0195\n",
      "Epoch 32: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.2500 - loss: 2.0195 - val_accuracy: 0.2727 - val_loss: 2.2711\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1719 - loss: 2.1115\n",
      "Epoch 33: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.1719 - loss: 2.1115 - val_accuracy: 0.2727 - val_loss: 2.2686\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.2812 - loss: 1.9619\n",
      "Epoch 34: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.2812 - loss: 1.9619 - val_accuracy: 0.2727 - val_loss: 2.2665\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2969 - loss: 2.0070\n",
      "Epoch 35: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.2969 - loss: 2.0070 - val_accuracy: 0.2273 - val_loss: 2.2644\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3281 - loss: 1.9547\n",
      "Epoch 36: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.3281 - loss: 1.9547 - val_accuracy: 0.2273 - val_loss: 2.2614\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2656 - loss: 2.0213\n",
      "Epoch 37: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.2656 - loss: 2.0213 - val_accuracy: 0.2273 - val_loss: 2.2573\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3125 - loss: 1.9771\n",
      "Epoch 38: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.3125 - loss: 1.9771 - val_accuracy: 0.2273 - val_loss: 2.2520\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3125 - loss: 1.9936\n",
      "Epoch 39: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.3125 - loss: 1.9936 - val_accuracy: 0.2273 - val_loss: 2.2459\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3125 - loss: 1.9141\n",
      "Epoch 40: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.3125 - loss: 1.9141 - val_accuracy: 0.2273 - val_loss: 2.2394\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3281 - loss: 1.9318\n",
      "Epoch 41: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.3281 - loss: 1.9318 - val_accuracy: 0.2273 - val_loss: 2.2334\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2656 - loss: 1.9626\n",
      "Epoch 42: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.2656 - loss: 1.9626 - val_accuracy: 0.2273 - val_loss: 2.2273\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3594 - loss: 1.8820\n",
      "Epoch 43: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.3594 - loss: 1.8820 - val_accuracy: 0.2273 - val_loss: 2.2210\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3750 - loss: 1.8179\n",
      "Epoch 44: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.3750 - loss: 1.8179 - val_accuracy: 0.2273 - val_loss: 2.2147\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3594 - loss: 1.9153\n",
      "Epoch 45: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.3594 - loss: 1.9153 - val_accuracy: 0.2273 - val_loss: 2.2085\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2656 - loss: 1.8892\n",
      "Epoch 46: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.2656 - loss: 1.8892 - val_accuracy: 0.2273 - val_loss: 2.2023\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2656 - loss: 1.9567\n",
      "Epoch 47: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.2656 - loss: 1.9567 - val_accuracy: 0.2273 - val_loss: 2.1958\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3906 - loss: 1.8292\n",
      "Epoch 48: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.3906 - loss: 1.8292 - val_accuracy: 0.2273 - val_loss: 2.1890\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3906 - loss: 1.8089\n",
      "Epoch 49: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.3906 - loss: 1.8089 - val_accuracy: 0.2273 - val_loss: 2.1819\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3438 - loss: 1.7695\n",
      "Epoch 50: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.3438 - loss: 1.7695 - val_accuracy: 0.2273 - val_loss: 2.1748\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2812 - loss: 1.8866\n",
      "Epoch 51: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.2812 - loss: 1.8866 - val_accuracy: 0.2273 - val_loss: 2.1672\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3906 - loss: 1.7650\n",
      "Epoch 52: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.3906 - loss: 1.7650 - val_accuracy: 0.2273 - val_loss: 2.1599\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3281 - loss: 1.8494\n",
      "Epoch 53: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.3281 - loss: 1.8494 - val_accuracy: 0.2273 - val_loss: 2.1517\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3438 - loss: 1.8875\n",
      "Epoch 54: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.3438 - loss: 1.8875 - val_accuracy: 0.2273 - val_loss: 2.1433\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3438 - loss: 1.8062\n",
      "Epoch 55: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.3438 - loss: 1.8062 - val_accuracy: 0.2273 - val_loss: 2.1346\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3125 - loss: 1.7836\n",
      "Epoch 56: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.3125 - loss: 1.7836 - val_accuracy: 0.2273 - val_loss: 2.1262\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.3281 - loss: 1.8455\n",
      "Epoch 57: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.3281 - loss: 1.8455 - val_accuracy: 0.2727 - val_loss: 2.1177\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3594 - loss: 1.7594\n",
      "Epoch 58: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.3594 - loss: 1.7594 - val_accuracy: 0.2727 - val_loss: 2.1092\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3906 - loss: 1.7528\n",
      "Epoch 59: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.3906 - loss: 1.7528 - val_accuracy: 0.2727 - val_loss: 2.1002\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3438 - loss: 1.7448\n",
      "Epoch 60: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.3438 - loss: 1.7448 - val_accuracy: 0.2727 - val_loss: 2.0908\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4062 - loss: 1.7148\n",
      "Epoch 61: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.4062 - loss: 1.7148 - val_accuracy: 0.2727 - val_loss: 2.0812\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2969 - loss: 1.8450\n",
      "Epoch 62: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.2969 - loss: 1.8450 - val_accuracy: 0.2727 - val_loss: 2.0716\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3438 - loss: 1.7636\n",
      "Epoch 63: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.3438 - loss: 1.7636 - val_accuracy: 0.2727 - val_loss: 2.0621\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3281 - loss: 1.7319\n",
      "Epoch 64: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.3281 - loss: 1.7319 - val_accuracy: 0.2727 - val_loss: 2.0521\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4531 - loss: 1.6805\n",
      "Epoch 65: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.4531 - loss: 1.6805 - val_accuracy: 0.2727 - val_loss: 2.0422\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3281 - loss: 1.8284\n",
      "Epoch 66: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.3281 - loss: 1.8284 - val_accuracy: 0.2727 - val_loss: 2.0318\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3594 - loss: 1.7042\n",
      "Epoch 67: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.3594 - loss: 1.7042 - val_accuracy: 0.2727 - val_loss: 2.0215\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4062 - loss: 1.6224\n",
      "Epoch 68: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.4062 - loss: 1.6224 - val_accuracy: 0.2727 - val_loss: 2.0108\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4062 - loss: 1.6296\n",
      "Epoch 69: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.4062 - loss: 1.6296 - val_accuracy: 0.2727 - val_loss: 2.0002\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3594 - loss: 1.6778\n",
      "Epoch 70: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.3594 - loss: 1.6778 - val_accuracy: 0.2727 - val_loss: 1.9896\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3281 - loss: 1.7009\n",
      "Epoch 71: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.3281 - loss: 1.7009 - val_accuracy: 0.2727 - val_loss: 1.9796\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.4375 - loss: 1.6018\n",
      "Epoch 72: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.4375 - loss: 1.6018 - val_accuracy: 0.2727 - val_loss: 1.9700\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3594 - loss: 1.7350\n",
      "Epoch 73: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.3594 - loss: 1.7350 - val_accuracy: 0.2727 - val_loss: 1.9613\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3750 - loss: 1.6196\n",
      "Epoch 74: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.3750 - loss: 1.6196 - val_accuracy: 0.2727 - val_loss: 1.9523\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4375 - loss: 1.6513\n",
      "Epoch 75: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.4375 - loss: 1.6513 - val_accuracy: 0.2727 - val_loss: 1.9434\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3281 - loss: 1.7037\n",
      "Epoch 76: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.3281 - loss: 1.7037 - val_accuracy: 0.2727 - val_loss: 1.9339\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3906 - loss: 1.5740\n",
      "Epoch 77: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.3906 - loss: 1.5740 - val_accuracy: 0.2727 - val_loss: 1.9245\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.3594 - loss: 1.5964\n",
      "Epoch 78: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3594 - loss: 1.5964 - val_accuracy: 0.2727 - val_loss: 1.9153\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3594 - loss: 1.6814\n",
      "Epoch 79: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.3594 - loss: 1.6814 - val_accuracy: 0.2727 - val_loss: 1.9065\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4219 - loss: 1.6129\n",
      "Epoch 80: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.4219 - loss: 1.6129 - val_accuracy: 0.2727 - val_loss: 1.8978\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4531 - loss: 1.4708\n",
      "Epoch 81: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.4531 - loss: 1.4708 - val_accuracy: 0.2727 - val_loss: 1.8892\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3594 - loss: 1.6078\n",
      "Epoch 82: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.3594 - loss: 1.6078 - val_accuracy: 0.2727 - val_loss: 1.8815\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4531 - loss: 1.5541\n",
      "Epoch 83: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.4531 - loss: 1.5541 - val_accuracy: 0.2727 - val_loss: 1.8739\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5156 - loss: 1.4748\n",
      "Epoch 84: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.5156 - loss: 1.4748 - val_accuracy: 0.2727 - val_loss: 1.8661\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.3594 - loss: 1.5668\n",
      "Epoch 85: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.3594 - loss: 1.5668 - val_accuracy: 0.2727 - val_loss: 1.8577\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4375 - loss: 1.5308\n",
      "Epoch 86: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.4375 - loss: 1.5308 - val_accuracy: 0.2727 - val_loss: 1.8495\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3750 - loss: 1.5980\n",
      "Epoch 87: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.3750 - loss: 1.5980 - val_accuracy: 0.2727 - val_loss: 1.8411\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2969 - loss: 1.6463\n",
      "Epoch 88: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.2969 - loss: 1.6463 - val_accuracy: 0.2727 - val_loss: 1.8328\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3906 - loss: 1.5272\n",
      "Epoch 89: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.3906 - loss: 1.5272 - val_accuracy: 0.2727 - val_loss: 1.8249\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4062 - loss: 1.4972\n",
      "Epoch 90: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.4062 - loss: 1.4972 - val_accuracy: 0.2727 - val_loss: 1.8164\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.2969 - loss: 1.6107\n",
      "Epoch 91: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.2969 - loss: 1.6107 - val_accuracy: 0.2727 - val_loss: 1.8076\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3906 - loss: 1.5084\n",
      "Epoch 92: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.3906 - loss: 1.5084 - val_accuracy: 0.2727 - val_loss: 1.7983\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4062 - loss: 1.4505\n",
      "Epoch 93: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.4062 - loss: 1.4505 - val_accuracy: 0.2727 - val_loss: 1.7899\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3906 - loss: 1.5582\n",
      "Epoch 94: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.3906 - loss: 1.5582 - val_accuracy: 0.2727 - val_loss: 1.7816\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3906 - loss: 1.4617\n",
      "Epoch 95: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.3906 - loss: 1.4617 - val_accuracy: 0.3182 - val_loss: 1.7737\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4219 - loss: 1.4801\n",
      "Epoch 96: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.4219 - loss: 1.4801 - val_accuracy: 0.3182 - val_loss: 1.7656\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4062 - loss: 1.3970\n",
      "Epoch 97: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4062 - loss: 1.3970 - val_accuracy: 0.3182 - val_loss: 1.7575\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4219 - loss: 1.5040\n",
      "Epoch 98: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.4219 - loss: 1.5040 - val_accuracy: 0.3182 - val_loss: 1.7496\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3438 - loss: 1.5211\n",
      "Epoch 99: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.3438 - loss: 1.5211 - val_accuracy: 0.3182 - val_loss: 1.7419\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4219 - loss: 1.5135\n",
      "Epoch 100: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.4219 - loss: 1.5135 - val_accuracy: 0.3182 - val_loss: 1.7340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1de0c02d670>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3182 - loss: 1.7340\n"
     ]
    }
   ],
   "source": [
    "# モデル評価\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存したモデルのロード\n",
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "[0.14375468 0.21153602 0.1256461  0.12431057 0.08514095 0.12822086\n",
      " 0.04370913 0.04578803 0.05073116 0.04116251]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 推論テスト\n",
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAH/CAYAAABq/LZGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzBUlEQVR4nO3de3hU1b3/8c+QhCGEEIUkEEQQsRKBBspFQBHlIpofInieoj8PaMRTT7URwfygkl4MCBoUC9gjgkVKaBVB7cFbVQ5CAXMgmosEQUXACoKQEMUEogy5zO8PbGQkziUzk7X38H757Odp9mTW/vLtfvTLd629tsPtdrsFAABgQAvTAQAAgHMXhQgAADCGQgQAABhDIQIAAIyhEAEAAMZQiAAAAGMoRAAAgDEUIgAAwBgKEQAAYAyFCAAAMIZCBAAANNnx48c1bdo0de3aVbGxsbriiitUWFjo9/cpRAAAQJP94he/0Pr16/XXv/5VH3zwgUaPHq1Ro0bp0KFDfn3fwUvvAABAU3z77beKj4/XK6+8ojFjxjSc79+/v9LT0zV37lyfY0SHM0AAAGAvLpdLLpfL45zT6ZTT6Tzrd2tra1VXV6dWrVp5nI+NjVV+fr5f17NMR6RXh0GmQ7C0x3Sx6RAsbdyxLaZDAHCOqz3l31REKNRUfBq2sR9+8i+aPXu2x7mcnBzNmjWr0d+/4oor1LJlS61atUodOnTQ888/r4yMDF1yySXavXu3z+tRiNgEhYh3FCIATIuUQqQ+/gK/OyKStG/fPt15553asmWLoqKi1K9fP1166aUqLi7WRx995PN6TM0AAGA39XVhG9pb0dGY7t27a/PmzaqurlZVVZVSUlJ0yy236OKL/fsLNE/NAACAoMXFxSklJUXHjh3TunXrNG7cOL++R0cEAAC7cdebjqDBunXr5Ha71aNHD+3du1czZsxQamqqJk+e7Nf36YgAAIAmq6ysVGZmplJTU3X77bdr6NChWrdunWJiYvz6Ph0RAADspt46HZGbb75ZN998c5O/TyECAIDNuC00NRMspmYAAIAxdEQAALAbC03NBIuOCAAAMIaOCAAAdsMaEQAAgODREQEAwG7CuMV7c6MjAgAAjKEjAgCA3bBGBAAAIHh0RAAAsJsI2keEQgQAAJthi3cAAIAQoCMCAIDdRNDUDB0RAABgDB0RAADshjUiAAAAwaMjAgCA3bDFOwAAQPDoiAAAYDcRtEaEQgQAALvh8V0AAIDg0REBAMBuImhqho4IAAAwho4IAAB2wxoRAACA4FGInKH/4L5a/NfH9Y/S17Wr7F2NSB9mOiRL6z7lRo0pe14959xuOhRLuefuDO39pEAnqvZpa/5rGjigr+mQLIX8+EaOvCM/kttdF7ajuVGInCG2dax279qjuTPnmw7F8hL6Xqwut49U1a79pkOxlAkTbtTj83M0Z+4CDRx0vUp3fKg3/v6ckpLamw7NEsiPb+TIO/ITeShEzpC/cZv+OO9pbXhzs+lQLC2qtVN9n7pXO/7fMtV8XW06HEu5f+pdemb5Kq38ywv66KM9+lXmTH3zzbeafMf/NR2aJZAf38iRd+TnO+768B3NLOBCpKKiQo899phuuukmDRkyREOGDNFNN92k+fPn6+jRo+GIERbTe96dKn/7fX25ZafpUCwlJiZG/fqlacPGdxrOud1ubdiYr8GD+xuMzBrIj2/kyDvyc4b6+vAdzSygQqSwsFCXXnqp/vjHPyohIUHDhg3TsGHDlJCQoD/+8Y9KTU1VUVFRuGKFBaSMH6K2aRdp98OrTYdiOYmJ7RQdHa3ysgqP8+XlR9WxQ5KhqKyD/PhGjrwjP5EpoMd3p0yZogkTJmjp0qVyOBwen7ndbt19992aMmWKtm3b5nUcl8sll8vlca7eXa8WDmaKrKxVp3bqNTdD7978iOpdNabDAYBzVwRtaBZQIVJaWqq8vLyzihBJcjgcuv/++/Wzn/3M5zi5ubmaPXu2x7nE1p2U3KZzIOGgmSX0uVjOpAQNXf9Iw7kW0VFqNyRVXe8crTcvvE2qdxuM0KyKiq9UW1ur5A6JHueTk5N0pIxpS/LjGznyjvxEpoBaEB07dtR77733o5+/99576tChg89xsrOzVVlZ6XEkxnUKJBQYULFlpzZfPUPvjJzZcHz9/j4d+tv/6p2RM8/pIkSSampqVFKyQyOGD20453A4NGL4UBUUFBuMzBrIj2/kyDvyc4b6uvAdzSygjsj06dP1n//5nyouLtbIkSMbio6ysjJt2LBBy5Yt0+OPP+5zHKfTKafT6XHOCtMyrVvHqku377synbt0Umqvn6jy6yodPlRmMDJrqKs+qRMfH/Q8941LNcdOnHX+XLXwiWVasXyhikt2qLDwfd035S7FxcUqb+Ua06FZAvnxjRx5R34iT0CFSGZmphITE7Vw4UI99dRTqqs7XTlFRUWpf//+ysvL08033xyWQJtDr76XKW/tkoafH3jofknSy6tf12+nzjEVFmzkxRdfVVJiO816cLo6dkxSaekujblhksrLK3x/+RxAfnwjR96Rn+9E0BoRh9vtblI/vaamRhUVp/+PT0xMVExMTFCB9OowKKjvR7rHdLHpECxt3LEtpkMAcI6rPXWo2a518r0XwzZ2q8snhG3sxjT5pXcxMTFKSUkJZSwAAMAfEfTSO96+CwCA3UTQ1Iz5FaIAAOCcRSECAIDdWGSL97q6Ov3+979Xt27dFBsbq+7du2vOnDkKZPkpUzMAAKBJHn30US1ZskQrV65Ur169VFRUpMmTJyshIUH33XefX2NQiAAAYDcWWay6detWjRs3TmPGjJEkXXTRRXr++ee9bn76Q0zNAACAJrniiiu0YcMGffLJJ5JOvwomPz9f6enpfo9BRwQAAJtxu8O3FXtjL6ZtbEd0SZo5c6aqqqqUmpqqqKgo1dXV6eGHH9bEiRP9vh4dEQAA0CA3N1cJCQkeR25ubqO/+8ILL+i5557TqlWrVFJSopUrV+rxxx/XypUr/b5ek3dWDTV2VvWOnVW9Y2dVAKY1586q3276c9jGbjFkot8dkQsvvFAzZ85UZmZmw7m5c+fq2Wef1ccff+zX9ZiaAQDAbsK4odmPFR2N+eabb9SihefkSlRUlOoDWExLIQIAAJpk7Nixevjhh9WlSxf16tVL77//vhYsWKA777zT7zEoRAAAsBuLPL77X//1X/r973+vX/3qVyovL1enTp30y1/+Ug8++KDfY1CIAACAJomPj9eiRYu0aNGiJo9BIQIAgN3w0jsAAIDg0REBAMBuLLJGJBToiAAAAGPoiAAAYDcRtEaEQgQAALthagYAACB4dEQAALAbOiIAAADBoyMCAIDdRNBiVToiAADAGDoiAADYDWtEAAAAgkdHBAAAu4mgNSIUIgAA2A1TMwAAAMGjIwIAgN1E0NQMHREAAGAMHREAAOwmgtaIWKYQ2X3soOkQLG17yiWmQwAAIOQsU4gAAAA/RVBHhDUiAADAGDoiAADYjdttOoKQoRABAMBumJoBAAAIHh0RAADsho4IAABA8OiIAABgN2zxDgAAEDw6IgAA2A1rRAAAAIJHRwQAALuJoA3N6IgAAABj6IgAAGA3EbRGhEIEAAC7iaBChKkZAABgDB0RAADshg3NAAAAgkdHBAAAm3HX8/guAABA0OiIAABgNzw1AwAAznUXXXSRHA7HWUdmZqbfY9ARAQDAbizy1ExhYaHq6uoaft65c6euvfZaTZgwwe8xKEQAALAbiyxWTUpK8vh53rx56t69u66++mq/x6AQAQAADVwul1wul8c5p9Mpp9Pp9XunTp3Ss88+q6ysLDkcDr+vxxoRAADspr4+bEdubq4SEhI8jtzcXJ8hvfzyy/r66691xx13BPRHoSMCAAAaZGdnKysry+Ocr26IJC1fvlzp6enq1KlTQNejEAEAwG7C+PiuP9MwP7R//369/fbb+u///u+Ar8fUDAAACMqKFSuUnJysMWPGBPxdOiIAANiN2xpPzUhSfX29VqxYoYyMDEVHB15W0BEBAABN9vbbb+vAgQO68847m/R9OiIAANiNhbZ4Hz16tNxBdGjoiPzAPXdnaO8nBTpRtU9b81/TwAF9TYdkGf0mjdQv3srV9J3PaPrOZ5Sxdpa6X9PHdFiWwz3kHfnxjRx5R350ekOzcB3NjELkDBMm3KjH5+doztwFGjjoepXu+FBv/P05JSW1Nx2aJRw//JX+8ehqLb/ht/rz2N9p/9ZdmrAsS4k/ucB0aJbBPeQd+fGNHHlHfiKPwx1MPyWEolua/4/Z1vzXVFhUqqnTfidJcjgc+uzTQi1+aoUem7/YaGyzU64xev0fk1X6tDY8skqlazYbjSPn8Caj1/8XK99DVkB+fCNH3lk5P7WnDjXbtb6Z37T1GP5oPePPYRu7MXREvhMTE6N+/dK0YeM7Defcbrc2bMzX4MH9DUZmTY4WDvUcO1gxsU4dKtlrOhxL4B7yjvz4Ro68Iz+RKeSFyOeff97klbMmJSa2U3R0tMrLKjzOl5cfVccOST/yrXNPUo8LNePD5Zq5Z6XSH75TL/1yoSr2NN/fAqyMe8g78uMbOfKO/JyBNSI/7quvvtLKlSu9/o7L5VJVVZXHYZEZIvjw5adf6Jn032jFuAdV/OwGjf3D3awRAQA0WcCP77766qteP//00099jpGbm6vZs2d7nHO0aCNHVNtAwwmZioqvVFtbq+QOiR7nk5OTdKTsqKGorKe+pk7H9pdJko7s/Eyd+lysgZOv05u/ad45RSviHvKO/PhGjrwjP99zW+jx3WAF3BEZP368brrpJo0fP77R44cvymlMdna2KisrPQ5Hi/gm/QFCpaamRiUlOzRi+NCGcw6HQyOGD1VBQbHByKzN0cKhqJYxpsOwBO4h78iPb+TIO/ITmQLuiKSkpOipp57SuHHjGv18+/bt6t/f+6Khxl6o43A4Ag0l5BY+sUwrli9UcckOFRa+r/um3KW4uFjlrVxjOjRLuObXt2jfplJVfVGhlnGx6jXuCnUdfJmev+1R06FZBveQd+THN3LkHfn5joG1HOEScCHSv39/FRcX/2gh4nA4bLve48UXX1VSYjvNenC6OnZMUmnpLo25YZLKyyt8f/kcEJfYVjcuuFttks+T6/g3Kv/4cz1/26P6Z/5O06FZBveQd+THN3LkHfn5jjtypmYC3kfknXfeUXV1ta6//vpGP6+urlZRUZGuvvrqgAKxwj4iVmbVfUSswir7iAA4dzXnPiLVcyeFbey43z0btrEbE3BH5KqrrvL6eVxcXMBFCAAACEAETc2woRkAADCGt+8CAGA35/LjuwAAAKFCRwQAALthjQgAAEDw6IgAAGA3EbSPCIUIAAB2w9QMAABA8OiIAABgM+f023cBAABChY4IAAB2wxoRAACA4NERAQDAbuiIAAAABI+OCAAAdsOGZgAAwBimZgAAAIJHRwQAAJtx0xEBAAAIHh0RAADsho4IAABA8OiIAABgN7z0DgAAIHh0RAAAsJsIWiNCIQIAgN1EUCHC1AwAADCGjggAADbjdtMRAQAACBqFCAAAdlPvDt8RoEOHDmnSpElq3769YmNj9dOf/lRFRUV+f5+pGQAA0CTHjh3TlVdeqeHDh+vNN99UUlKS9uzZo/PPP9/vMShEAACwG4s8NfPoo4/qwgsv1IoVKxrOdevWLaAxmJoBAAANXC6XqqqqPA6Xy9Xo77766qsaMGCAJkyYoOTkZP3sZz/TsmXLArqew22RpbfRLS8wHYKl9Ti/s+kQLG33sYOmQwBwjqs9dajZrlU5eVTYxl7Ydahmz57tcS4nJ0ezZs0663dbtWolScrKytKECRNUWFioqVOnaunSpcrIyPDrehQiNkEh4h2FCADTmrUQyRgZtrFb/emNszogTqdTTqfzrN9t2bKlBgwYoK1btzacu++++1RYWKht27b5dT3WiAAAgAY/VnQ0JiUlRT179vQ4d9lll+lvf/ub39ejEAEAwG4s8vLdK6+8Urt37/Y498knn6hr165+j8FiVQAA0CT333+/CgoK9Mgjj2jv3r1atWqV/vSnPykzM9PvMeiIAABgM26LPL47cOBArV27VtnZ2XrooYfUrVs3LVq0SBMnTvR7DAoRAADQZDfccINuuOGGJn+fQgQAALuxSEckFFgjAgAAjKEjAgCA3VjkqZlQoCMCAACMoSMCAIDNWOWpmVCgEAEAwG6YmgEAAAgeHREAAGwmkqZm6IgAAABj6IgAAGA3rBEBAAAIHh0RAABsxk1HBAAAIHh0RAAAsJsI6ohQiAAAYDNMzQAAAIQAHREAAOyGjggAAEDw6IgAAGAzrBEBAAAIAToiAADYDB0RAACAEKAjAgCAzURSR4RCBAAAu3E7TEcQMkzNAAAAY+iIAABgM5E0NUNH5AfuuTtDez8p0Imqfdqa/5oGDuhrOiTL6D+4rxb/9XH9o/R17Sp7VyPSh5kOyZK4h7wjP76RI+/IT2ShEDnDhAk36vH5OZozd4EGDrpepTs+1Bt/f05JSe1Nh2YJsa1jtXvXHs2dOd90KJbFPeQd+fGNHHlHfk5z1zvCdjQ3h9vtdjf7VRsR3fIC0yFoa/5rKiwq1dRpv5MkORwOffZpoRY/tUKPzV9sNLYe53c2ev0f2lX2rqbcMUMb39xiOhRJ0u5jB02HIMna95AVkB/fyJF3Vs5P7alDzXatw0OHh23slPx/hG3sxtAR+U5MTIz69UvTho3vNJxzu93asDFfgwf3NxgZ7IJ7yDvy4xs58o78fM9dH76juQVciHz77bfKz8/Xhx9+eNZnJ0+e1F/+8peQBNbcEhPbKTo6WuVlFR7ny8uPqmOHJENRwU64h7wjP76RI+/IT2QKqBD55JNPdNlll2nYsGH66U9/qquvvlqHDx9u+LyyslKTJ0/2OY7L5VJVVZXHYZEZIgAALM/tdoTtaG4BFSIPPPCAevfurfLycu3evVvx8fG68sordeDAgYAumpubq4SEBI/DXX88oDFCraLiK9XW1iq5Q6LH+eTkJB0pO2ooKtgJ95B35Mc3cuQd+fneOTs1s3XrVuXm5ioxMVGXXHKJXnvtNV133XW66qqr9Omnn/o9TnZ2tiorKz0OR4v4gIMPpZqaGpWU7NCI4UMbzjkcDo0YPlQFBcUGI4NdcA95R358I0fekZ/IFNCGZt9++62io7//isPh0JIlS3Tvvffq6quv1qpVq/wax+l0yul0epxzOMxvV7vwiWVasXyhikt2qLDwfd035S7FxcUqb+Ua06FZQuvWserS7fundzp36aTUXj9R5ddVOnyozGBk1sE95B358Y0ceUd+TjPxmG24BFSIpKamqqioSJdddpnH+SeffFKSdOONN4YuMgNefPFVJSW206wHp6tjxySVlu7SmBsmqby8wveXzwG9+l6mvLVLGn5+4KH7JUkvr35dv506x1RYlsI95B358Y0ceUd+Ik9A+4jk5ubqnXfe0RtvvNHo57/61a+0dOlS1dcHPslkhX1ErMxq+4hYjVX2EQFw7mrOfUQODBgZtrG7FG0I29iNYUMzm6AQ8Y5CBIBpFCJNw0vvAACwmUhaI8LOqgAAwBgKEQAAbMYqL72bNWuWHA6Hx5GamhrQGEzNAABgM9ZY3Xlar1699Pbbbzf8fOY2H/6gEAEAAE0WHR2tjh07Nvn7TM0AAGAzVpmakaQ9e/aoU6dOuvjiizVx4sSAX/tCRwQAADRwuVxyuVwe5xrbEV2SBg0apLy8PPXo0UOHDx/W7NmzddVVV2nnzp2Kj/fv1S10RAAAsJlwvn23sRfT5ubmNhpHenq6JkyYoLS0NF133XV644039PXXX+uFF17w+89CRwQAADTIzs5WVlaWx7nGuiGNOe+883TppZdq7969fl+PQgQAAJtxB/4mFb/92DSMP06cOKF9+/bptttu8/s7TM0AAIAmmT59ujZv3qzPPvtMW7du1U033aSoqCjdeuutfo9BRwQAAJupd1tji/eDBw/q1ltv1ZdffqmkpCQNHTpUBQUFSkpK8nsMChEAAGzGbZFCZPXq1UGPwdQMAAAwho4IAAA2w9t3AQAAQoCOCAAANmOll94Fi44IAAAwho4IAAA2wxoRAACAEKAjAgCAzVhlQ7NQoBABAMBmrLKhWSgwNQMAAIyhIwIAgM3w+C4AAEAI0BEBAMBmImmxKh0RAABgDB0RAABshqdmAAAAQoCOCAAANhNJT81QiAAAYDMsVgUAAAgBOiI2sfvYQdMhwOZmp1xjOgRLyzm8yXQIgN9YrAoAABACdEQAALAZ1ogAAACEAB0RAABsJoKe3qUjAgAAzKEjAgCAzUTSGhEKEQAAbIbHdwEAAEKAjggAADZTbzqAEKIjAgAAjKEjAgCAzbjFGhEAAICg0REBAMBm6iNoRzM6IgAAwBg6IgAA2Ew9a0QAAACCR0cEAACbiaSnZihEAACwGTY0AwAACAE6IgAA2EwkTc3QEQEAAMZQiAAAYDP1YTyaat68eXI4HJo2bVpA36MQAQAAQSksLNTTTz+ttLS0gL9LIQIAgM1YqSNy4sQJTZw4UcuWLdP5558f8PcpRAAAQAOXy6WqqiqPw+Vy/ejvZ2ZmasyYMRo1alSTrkchAgCAzbjlCNuRm5urhIQEjyM3N7fROFavXq2SkpIf/dwfPL4LAIDN1Ifx6d3s7GxlZWV5nHM6nWf93ueff66pU6dq/fr1atWqVZOvRyECAAAaOJ3ORguPHyouLlZ5ebn69evXcK6urk5btmzRk08+KZfLpaioKJ/jUIgAAGAzVnj77siRI/XBBx94nJs8ebJSU1P1wAMP+FWESBQiAACgCeLj49W7d2+Pc3FxcWrfvv1Z572hEAEAwGbcpgMIIQoRAAAQEps2bQr4Ozy++wP33J2hvZ8U6ETVPm3Nf00DB/Q1HZLlkCPvyM+P6zdppH7xVq6m73xG03c+o4y1s9T9mj6mw7Ic7iHvyI+1NjQLFoXIGSZMuFGPz8/RnLkLNHDQ9Srd8aHe+PtzSkpqbzo0yyBH3pEf744f/kr/eHS1lt/wW/157O+0f+suTViWpcSfXGA6NMvgHvKO/EQeh9vttsRUU3RL8/8i2pr/mgqLSjV12u8kSQ6HQ599WqjFT63QY/MXG47OGsiRd1bOz+yUa4xe/8dklT6tDY+sUumazUbjyDm8yej1/8XK95AVWDk/tacONdu1XkqZGLaxf374ubCN3Rg6It+JiYlRv35p2rDxnYZzbrdbGzbma/Dg/gYjsw5y5B35CYyjhUM9xw5WTKxTh0r2mg7HEriHvCM/33OH8WhuAS9W/eijj1RQUKAhQ4YoNTVVH3/8sZ544gm5XC5NmjRJI0aMCEecYZeY2E7R0dEqL6vwOF9eflSpPbobispayJF35Mc/ST0u1B1rZynaGaNT1Sf10i8XqmJP8/1N0sq4h7wjP5EpoELkrbfe0rhx49SmTRt98803Wrt2rW6//Xb16dNH9fX1Gj16tP7nf/7HZzHicrnOeoGO2+2Ww2F+gxYA4fXlp1/omfTfyBkfq9T/M0hj/3C3nr1lLsUIEAATi0rDJaCpmYceekgzZszQl19+qRUrVujf//3fddddd2n9+vXasGGDZsyYoXnz5vkcp7EX6rjrjzf5DxEKFRVfqba2VskdEj3OJycn6UjZUUNRWQs58o78+Ke+pk7H9pfpyM7PtOmxNSr/6IAGTr7OdFiWwD3kHfmJTAEVIrt27dIdd9whSbr55pt1/Phx/fznP2/4fOLEidqxY4fPcbKzs1VZWelxOFrEBxZ5iNXU1KikZIdGDB/acM7hcGjE8KEqKCg2GJl1kCPvyE/TOFo4FNUyxnQYlsA95B35+V69I3xHcwt4jci/pk9atGihVq1aKSEhoeGz+Ph4VVZW+hyjsRfqWGFaZuETy7Ri+UIVl+xQYeH7um/KXYqLi1XeyjWmQ7MMcuQd+fHuml/fon2bSlX1RYVaxsWq17gr1HXwZXr+tkdNh2YZ3EPekZ/IE1AhctFFF2nPnj3q3v30oqBt27apS5cuDZ8fOHBAKSkpoY2wGb344qtKSmynWQ9OV8eOSSot3aUxN0xSeXmF7y+fI8iRd+THu7jEtrpxwd1qk3yeXMe/UfnHn+v52x7VP/N3mg7NMriHvCM/p1nhpXehEtA+IkuXLtWFF16oMWPGNPr5b37zG5WXl+uZZ54JOBAr7CMCRDKr7iNiFVbZRwT21Zz7iDzXaVLYxp74xbNhG7sxAXVE7r77bq+fP/LII0EFAwAAfLPETqQhwkvvAACwGROLSsOFnVUBAIAxdEQAALCZc3ZDMwAAgFCiIwIAgM1E0mJVOiIAAMAYOiIAANgMT80AAACEAB0RAABsJpKemqEQAQDAZiKpEGFqBgAAGENHBAAAm3GzWBUAACB4dEQAALAZ1ogAAACEAB0RAABsho4IAABACNARAQDAZiLppXcUIgAA2AzvmgEAAAgBOiIAANgMi1UBAABCgI4IAAA2Q0cEAAAgBOiIAABgM5H0+C4dEQAAYAwdEQAAbCaS9hGhEAEAwGZYrAoAAM55S5YsUVpamtq2bau2bdtqyJAhevPNNwMag0IEAACbcYfxCETnzp01b948FRcXq6ioSCNGjNC4ceO0a9cuv8dgagYAADTJ2LFjPX5++OGHtWTJEhUUFKhXr15+jUEhAgCAzdSH8QFel8sll8vlcc7pdMrpdHr9Xl1dnV588UVVV1dryJAhfl+PQsQmvv3iHdMhWFpsp6tMh2B5OYc3mQ4BgA3k5uZq9uzZHudycnI0a9asRn//gw8+0JAhQ3Ty5Em1adNGa9euVc+ePf2+nsPtdltiX5TolheYDsHSKES8oxABYFrtqUPNdq05XSeGbexff/LngDoip06d0oEDB1RZWamXXnpJzzzzjDZv3ux3MUJHBAAANPBnGuZMLVu21CWXXCJJ6t+/vwoLC/XEE0/o6aef9uv7FCIAANiMJaYyfkR9ff1ZHRVvKEQAALAZq2xolp2drfT0dHXp0kXHjx/XqlWrtGnTJq1bt87vMShEAABAk5SXl+v222/X4cOHlZCQoLS0NK1bt07XXnut32NQiAAAYDNWedfM8uXLgx6DnVUBAIAxdEQAALCZcG5o1tzoiAAAAGPoiAAAYDOR0w+hIwIAAAyiIwIAgM1YZR+RUKAjAgAAjKEjAgCAzUTSUzMUIgAA2EzklCFMzQAAAIPoiAAAYDMsVgUAAAgBOiIAANhMJC1WpSMCAACMoSMCAIDNRE4/hI4IAAAwiI4IAAA2E0lPzVCIAABgM+4ImpxhagYAABhDRwQAAJuJpKkZOiIAAMAYOiIAANgMG5oBAACEAB0RAABsJnL6IXREAACAQXREAACwGdaIRLB77s7Q3k8KdKJqn7bmv6aBA/qaDslSqqu/0bxFS3Xtv2Wo//BxmvjLLH3w0W7TYVkK95B35Mc3cuQd+Tn9+G64juZGIXKGCRNu1OPzczRn7gINHHS9Snd8qDf+/pySktqbDs0yHpz3hLYVvq/cB6dr7V+X6IrL++muqb9R2dEK06FZAveQd+THN3LkHfmJPA632x10f8ftdsvhcAQ1RnTLC4INI2hb819TYVGppk77nSTJ4XDos08LtfipFXps/mKjsX37xTtGry9JJ10uDbr23/THeTm6+orLG87ffOcUDR08QPf9Z4ax2GI7XWXs2mey8j1kBeTHN3LknZXzU3vqULNd6xcX/TxsYz/z2UthG7sxIemIOJ1OffTRR6EYypiYmBj165emDRu//w++2+3Who35Gjy4v8HIrKOutk51dfVytozxOO90tlTJjl2GorIO7iHvyI9v5Mg78hOZAlqsmpWV1ej5uro6zZs3T+3bn26NLViwIPjImlliYjtFR0ervMxziqG8/KhSe3Q3FJW1xMW1Vp/el2lp3vO6uGsXtW93nt54e7NKd36sLhekmA7POO4h78iPb+TIO/LzvUja4j2gQmTRokXq06ePzjvvPI/zbrdbH330keLi4vyaonG5XHK5XGeNEez0DsIv9/fT9WDuQo0YP0lRUS102aWXKH3U1fpw917ToQEAbCigQuSRRx7Rn/70J/3hD3/QiBEjGs7HxMQoLy9PPXv29Guc3NxczZ492+Oco0UbOaLaBhJOSFVUfKXa2lold0j0OJ+cnKQjZUcNRWU9XTp3Ut7i+frm25Oqrv5GSYnt9P9+n6vOnTqaDs047iHvyI9v5Mg78vM997n6+O7MmTO1Zs0a3XPPPZo+fbpqamqadNHs7GxVVlZ6HI4W8U0aK1RqampUUrJDI4YPbTjncDg0YvhQFRQUG4zMmlrHtlJSYjtVVh3X1veKNeKqwaZDMo57yDvy4xs58o78RKaANzQbOHCgiouLlZmZqQEDBui5554LeErF6XTK6XR6nLPCtMzCJ5ZpxfKFKi7ZocLC93XflLsUFxervJVrTIdmGf/7brHcbrcu6tJZBw5+oT8sXq5uXTpr/JjRpkOzBO4h78iPb+TIO/Jz2jm7RuRf2rRpo5UrV2r16tUaNWqU6urqQh2XES+++KqSEttp1oPT1bFjkkpLd2nMDZNUXs4eGf9y/ES1Fi1dobKjFUpoG69rrx6q+36ZoZhoNumVuId8IT++kSPvyM9p9cHvvGEZQe8jcvDgQRUXF2vUqFGKi4tr8jhW2EfEyqywj4iVWWUfEQDnrubcR+S2rv8WtrH/uv+/wzZ2Y4L+a2znzp3VuXPnUMQCAAD8EDn9ELZ4BwAABjGxDwCAzfD2XQAAcM7Lzc3VwIEDFR8fr+TkZI0fP167dwf2RnYKEQAAbMYdxn8CsXnzZmVmZqqgoEDr169XTU2NRo8ererqar/HYGoGAAA0yVtvveXxc15enpKTk1VcXKxhw4b5NQaFCAAANmPVDc0qKyslSe3atfP7OxQiAADYTDgXqzb2YtrGdkQ/K6b6ek2bNk1XXnmlevfu7ff1WCMCAAAa5ObmKiEhwePIzc31+b3MzEzt3LlTq1evDuh6dEQAALCZcL59Nzs7W1lZWR7nfHVD7r33Xr3++uvasmVLwJucUogAAIAG/kzD/Ivb7daUKVO0du1abdq0Sd26dQv4ehQiAADYjFUWq2ZmZmrVqlV65ZVXFB8fryNHjkiSEhISFBsb69cYrBEBAABNsmTJElVWVuqaa65RSkpKw7FmzRq/x6AjAgCAzbjd1tjiPRRx0BEBAADG0BEBAMBmIumldxQiAADYjFUWq4YCUzMAAMAYOiIAANhMODc0a250RAAAgDF0RAAAsJlIWqxKRwQAABhDRwQAAJuxyoZmoUBHBAAAGENHBAAAm4mkfUQoRAAAsBke3wUAAAgBOiIAANgMj+8CAACEAB0RAABshsd3AQAAQoCOCAAANsMaEQAAgBCgI2ITfXvdajoEAIBFRNI+IhQiAADYTD2LVQEAAIJHRwQAAJuJnH4IHREAAGAQHREAAGyGx3cBAABCgI4IAAA2Q0cEAAAgBOiIAABgM7z0DgAAIAToiAAAYDORtEaEQgQAAJuJpHfNMDUDAACMoSMCAIDNsFgVAAAgBOiIAABgM5G0WJWOCAAAMIaOCAAANsMaEQAAgBCgIwIAgM1E0hoRChEAAGyGDc0AAMA5b8uWLRo7dqw6deokh8Ohl19+OeAxKEQAALCZerc7bEcgqqur1adPHy1evLjJfxamZgAAQJOkp6crPT09qDEoRAAAsJlwrhFxuVxyuVwe55xOp5xOZ1iux9QMAABokJubq4SEBI8jNzc3bNejIwIAgM0EupYjENnZ2crKyvI4F65uiEQhAgAAzhDOaZjGUIgAAGAzkbSPCIUIAAA2E86pmUCcOHFCe/fubfj5n//8p7Zv36527dqpS5cufo1BIQIAAJqkqKhIw4cPb/j5X2tLMjIylJeX59cYFCIAANiMVaZmrrnmmqDfBMzjuz9wz90Z2vtJgU5U7dPW/Nc0cEBf0yFZRv/BfbX4r4/rH6Wva1fZuxqRPsx0SJbEPeQd+fGNHHlHfiILhcgZJky4UY/Pz9GcuQs0cND1Kt3xod74+3NKSmpvOjRLiG0dq9279mjuzPmmQ7Es7iHvyI9v5Mg78nOaVbZ4DwWHO9ieSohEt7zAdAjamv+aCotKNXXa7yRJDodDn31aqMVPrdBj85u+j34o9Di/s9Hr/9Cusnc15Y4Z2vjmFtOhSJJ2HztoOgRJ1r6HrID8+EaOvLNyfmpPHWq2a3VP7Be2sfdVlIRt7MbQEflOTEyM+vVL04aN7zScc7vd2rAxX4MH9zcYGeyCe8g78uMbOfKO/HzPHcZ/mltQi1Wrq6v1wgsvaO/evUpJSdGtt96q9u3t2R5LTGyn6OholZdVeJwvLz+q1B7dDUUFO+Ee8o78+EaOvCM/kSmgQqRnz57Kz89Xu3bt9Pnnn2vYsGE6duyYLr30Uu3bt09z5sxRQUGBunXr5nWcxl6o43a75XA4Av8TAABwjnG7602HEDIBTc18/PHHqq2tlXR6L/pOnTpp//79eu+997R//36lpaXpt7/9rc9xGnuhjrv+eNP+BCFSUfGVamtrldwh0eN8cnKSjpQdNRQV7IR7yDvy4xs58o78fK9e7rAdza3Ja0S2bdumWbNmKSEhQZLUpk0bzZ49W/n5+T6/m52drcrKSo/D0SK+qaGERE1NjUpKdmjE8KEN5xwOh0YMH6qCgmKDkcEuuIe8Iz++kSPvyE9kCniNyL+mT06ePKmUlBSPzy644AIdPeq7Km3shTpWmJZZ+MQyrVi+UMUlO1RY+L7um3KX4uJilbdyjenQLKF161h16fb90zudu3RSaq+fqPLrKh0+VGYwMuvgHvKO/PhGjrwjP6dZ5IHXkAi4EBk5cqSio6NVVVWl3bt3q3fv3g2f7d+/37aLVSXpxRdfVVJiO816cLo6dkxSaekujblhksrLK3x/+RzQq+9lylu7pOHnBx66X5L08urX9dupc0yFZSncQ96RH9/IkXfkJ/IEtI/I7NmzPX4ePHiwrrvuuoafZ8yYoYMHD+r5558POBAr7CNiZVbbR8RqrLKPCIBzV3PuI9K5XW/fv9REB7/aGbaxG8OGZjZBIeIdhQgA0yhEmoaX3gEAYDMW6SGEBDurAgAAY+iIAABgMyZeThcuFCIAANiMiXfChAtTMwAAwBg6IgAA2AyLVQEAAEKAjggAADZj4uV04UJHBAAAGENHBAAAm2GNCAAAQAjQEQEAwGbY0AwAABjD1AwAAEAI0BEBAMBmeHwXAAAgBOiIAABgM6wRAQAACAE6IgAA2EwkPb5LRwQAABhDRwQAAJtxR9BTMxQiAADYDFMzAAAAIUBHBAAAm+HxXQAAgBCgIwIAgM1E0mJVOiIAAMAYOiIAANgMa0QAAAC+s3jxYl100UVq1aqVBg0apPfee8/v71KIAABgM263O2xHoNasWaOsrCzl5OSopKREffr00XXXXafy8nK/vk8hAgCAzbjDeARqwYIFuuuuuzR58mT17NlTS5cuVevWrfXnP//Zr+9TiAAAgAYul0tVVVUeh8vlavR3T506peLiYo0aNarhXIsWLTRq1Cht27bNvwu6cZaTJ0+6c3Jy3CdPnjQdiiWRH9/IkXfkxzvy4xs5Cp+cnJyzGiU5OTmN/u6hQ4fcktxbt271OD9jxgz35Zdf7tf1HG53BC29DZGqqiolJCSosrJSbdu2NR2O5ZAf38iRd+THO/LjGzkKH5fLdVYHxOl0yul0nvW7X3zxhS644AJt3bpVQ4YMaTj/61//Wps3b9a7777r83o8vgsAABr8WNHRmMTEREVFRamsrMzjfFlZmTp27OjXGKwRAQAATdKyZUv1799fGzZsaDhXX1+vDRs2eHRIvKEjAgAAmiwrK0sZGRkaMGCALr/8ci1atEjV1dWaPHmyX9+nEGmE0+lUTk6O362pcw358Y0ceUd+vCM/vpEj67jlllt09OhRPfjggzpy5Ij69u2rt956Sx06dPDr+yxWBQAAxrBGBAAAGEMhAgAAjKEQAQAAxlCIAAAAYyhEfiCYVxlHui1btmjs2LHq1KmTHA6HXn75ZdMhWUpubq4GDhyo+Ph4JScna/z48dq9e7fpsCxlyZIlSktLU9u2bdW2bVsNGTJEb775pumwLGvevHlyOByaNm2a6VAsYdasWXI4HB5Hamqq6bAQJAqRMwT7KuNIV11drT59+mjx4sWmQ7GkzZs3KzMzUwUFBVq/fr1qamo0evRoVVdXmw7NMjp37qx58+apuLhYRUVFGjFihMaNG6ddu3aZDs1yCgsL9fTTTystLc10KJbSq1cvHT58uOHIz883HRKCxOO7Zxg0aJAGDhyoJ598UtLp3eEuvPBCTZkyRTNnzjQcnbU4HA6tXbtW48ePNx2KZR09elTJycnavHmzhg0bZjocy2rXrp3mz5+v//iP/zAdimWcOHFC/fr101NPPaW5c+eqb9++WrRokemwjJs1a5Zefvllbd++3XQoCCE6It8JyauMgTNUVlZKOv0fWpytrq5Oq1evVnV1td9bQZ8rMjMzNWbMGI9/H+G0PXv2qFOnTrr44os1ceJEHThwwHRICBI7q36noqJCdXV1Z+0E16FDB3388ceGooJd1dfXa9q0abryyivVu3dv0+FYygcffKAhQ4bo5MmTatOmjdauXauePXuaDssyVq9erZKSEhUWFpoOxXIGDRqkvLw89ejRQ4cPH9bs2bN11VVXaefOnYqPjzcdHpqIQgQIg8zMTO3cuZP560b06NFD27dvV2VlpV566SVlZGRo8+bNFCOSPv/8c02dOlXr169Xq1atTIdjOenp6Q3/Oy0tTYMGDVLXrl31wgsvMLVnYxQi3wnFq4wBSbr33nv1+uuva8uWLercubPpcCynZcuWuuSSSyRJ/fv3V2FhoZ544gk9/fTThiMzr7i4WOXl5erXr1/Dubq6Om3ZskVPPvmkXC6XoqKiDEZoLeedd54uvfRS7d2713QoCAJrRL4TilcZ49zmdrt17733au3atdq4caO6detmOiRbqK+vl8vlMh2GJYwcOVIffPCBtm/f3nAMGDBAEydO1Pbt2ylCfuDEiRPat2+fUlJSTIeCINAROUOwrzKOdCdOnPD4m8c///lPbd++Xe3atVOXLl0MRmYNmZmZWrVqlV555RXFx8fryJEjkqSEhATFxsYajs4asrOzlZ6eri5duuj48eNatWqVNm3apHXr1pkOzRLi4+PPWlMUFxen9u3bs9ZI0vTp0zV27Fh17dpVX3zxhXJychQVFaVbb73VdGgIAoXIGYJ9lXGkKyoq0vDhwxt+zsrKkiRlZGQoLy/PUFTWsWTJEknSNddc43F+xYoVuuOOO5o/IAsqLy/X7bffrsOHDyshIUFpaWlat26drr32WtOhwQYOHjyoW2+9VV9++aWSkpI0dOhQFRQUKCkpyXRoCAL7iAAAAGNYIwIAAIyhEAEAAMZQiAAAAGMoRAAAgDEUIgAAwBgKEQAAYAyFCAAAMIZCBAAAGEMhAgAAjKEQAQAAxlCIAAAAYyhEAACAMf8f6ZUixdkGRL0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33         5\n",
      "           1       0.17      1.00      0.29         3\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       0.00      0.00      0.00         9\n",
      "           5       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.32        22\n",
      "   macro avg       0.36      0.37      0.27        22\n",
      "weighted avg       0.39      0.32      0.25        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunna\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gunna\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gunna\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-Lite用のモデルへ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to model for TensorFlow-Lite\n",
    "# Save as a model dedicated to inference\n",
    "model.save(model_save_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_get_save_spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tflite_save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(tflite_quantized_model)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mconvert_to_tflite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtflite_save_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 6\u001b[0m, in \u001b[0;36mconvert_to_tflite\u001b[1;34m(model_path, tflite_save_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n\u001b[0;32m      5\u001b[0m converter\u001b[38;5;241m.\u001b[39moptimizations \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mOptimize\u001b[38;5;241m.\u001b[39mDEFAULT]\n\u001b[1;32m----> 6\u001b[0m tflite_quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tflite_save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(tflite_quantized_model)\n",
      "File \u001b[1;32m~\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1175\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1174\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1129\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m   1128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m-> 1129\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[1;32m~\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1641\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[0;32m   1638\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n\u001b[0;32m   1640\u001b[0m graph_def, input_tensors, output_tensors, frozen_func \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1641\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_freeze_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1642\u001b[0m )\n\u001b[0;32m   1644\u001b[0m graph_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_tf_model(\n\u001b[0;32m   1645\u001b[0m     graph_def, input_tensors, output_tensors, frozen_func\n\u001b[0;32m   1646\u001b[0m )\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(TFLiteKerasModelConverterV2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m   1649\u001b[0m     graph_def, input_tensors, output_tensors\n\u001b[0;32m   1650\u001b[0m )\n",
      "File \u001b[1;32m~\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[1;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[1;32m~\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1582\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._freeze_keras_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;66;03m# If the model's call is not a `tf.function`, then we need to first get its\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;66;03m# input signature from `model_input_signature` method. We can't directly\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# call `trace_model_call` because otherwise the batch dimension is set\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;66;03m# to None.\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;66;03m# Once we have better support for dynamic shapes, we can remove this.\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_model\u001b[38;5;241m.\u001b[39mcall, _def_function\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[0;32m   1579\u001b[0m   \u001b[38;5;66;03m# Pass `keep_original_batch_size=True` will ensure that we get an input\u001b[39;00m\n\u001b[0;32m   1580\u001b[0m   \u001b[38;5;66;03m# signature including the batch dimension specified by the user.\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m   \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n\u001b[1;32m-> 1582\u001b[0m   input_signature \u001b[38;5;241m=\u001b[39m \u001b[43m_model_input_signature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_original_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   1584\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n\u001b[0;32m   1587\u001b[0m func \u001b[38;5;241m=\u001b[39m _trace_model_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_model, input_signature)\n",
      "File \u001b[1;32m~\\Downloads\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\tflite_keras_util.py:84\u001b[0m, in \u001b[0;36mmodel_input_signature\u001b[1;34m(model, keep_original_batch_size)\u001b[0m\n\u001b[0;32m     82\u001b[0m   input_specs \u001b[38;5;241m=\u001b[39m input_specs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m   input_specs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_save_spec\u001b[49m(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     85\u001b[0m       dynamic_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m keep_original_batch_size)\n\u001b[0;32m     86\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_specs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_get_save_spec'"
     ]
    }
   ],
   "source": [
    "# Transform model (quantization)\n",
    "def convert_to_tflite(model_path, tflite_save_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_quantized_model = converter.convert()\n",
    "    with open(tflite_save_path, 'wb') as f:\n",
    "        f.write(tflite_quantized_model)\n",
    "\n",
    "convert_to_tflite(model_save_path, tflite_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference test with TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get I/O tensor\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference implementation\n",
    "interpreter.invoke()\n",
    "# Get output tensor\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
